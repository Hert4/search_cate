# Intelligent Product Categorization System

This project implements multiple approaches for intelligent product categorization, matching product names to the most appropriate categories from a large set of possible categories. The system is designed to handle both small-scale and large-scale (100K+ categories) scenarios with different algorithmic approaches.

## ğŸ“‹ Table of Contents

- [Overview](#overview)
- [Methods Overview](#methods-overview)
- [Detailed Method Descriptions](#detailed-method-descriptions)
- [Architecture](#architecture)
- [Usage Examples](#usage-examples)
- [Performance Comparison](#performance-comparison)
- [Installation](#installation)

## Overview

The system provides 7 different methods for product categorization, each optimized for different use cases and data characteristics. The core idea is to match a product name to the most relevant category from a given list of categories using various search and ranking techniques.

## Methods Overview

The project implements 7 distinct approaches:

1. **Adaptive Priority + Lexical** - Uses prefix tree with lexical search
2. **Adaptive Priority + Semantic** - Uses prefix tree with semantic search  
3. **Adaptive Priority + BM25 Lexical** - Uses BM25 algorithm for lexical matching
4. **Optimized Pipeline + BM25** - Enhanced BM25 with cross-encoder reranking
5. **Inverted Index** - Scalable approach using inverted index
6. **N-gram Index** - Scalable approach using n-gram matching
7. **Hybrid** - Combines inverted index and semantic search

## Detailed Method Descriptions

### 1. Adaptive Priority + Lexical (`method_prefix_lexical_cross.py`)

**Approach**: Combines prefix tree matching with lexical search, enhanced with adaptive priority calculation

**Pipeline**:
1. Calculate adaptive priority for all categories based on specificity, token overlap, and length
2. Build prefix tree from prioritized categories
3. Use prefix matching to find initial candidates
4. If no prefix match found, use lexical search
5. Apply cross-encoder reranking to determine best match

**Use Case**: Good for scenarios where categories have common prefixes with the query

**Minh há»a cÃ¡ch hoáº¡t Ä‘á»™ng**:
```
Query: "iPhone 13 Pro Max"
Categories: ["iPhone", "iPhone 13", "iPhone 13 Pro", "iPhone 13 Pro Max", "Äiá»‡n thoáº¡i", "Äiá»‡n tá»­"]

BÆ°á»›c 1 - TÃ­nh Priority:
- "iPhone": specificity=0.1, overlap=1, length=1 â†’ priority=2.6
- "iPhone 13": specificity=0.2, overlap=2, length=2 â†’ priority=5.4
- "iPhone 13 Pro": specificity=0.3, overlap=3, length=3 â†’ priority=8.3
- "iPhone 13 Pro Max": specificity=0.4, overlap=4, length=4 â†’ priority=11.4
- "Äiá»‡n thoáº¡i": specificity=0.5, overlap=0, length=2 â†’ priority=1.5
- "Äiá»‡n tá»­": specificity=0.6, overlap=0, length=2 â†’ priority=1.6

BÆ°á»›c 2 - Build Prefix Tree:
                      root
                       |
                     iPhone â”€â”€â”¬â”€â”€ "iPhone" (priority=2.6)
                       |     â””â”€â”€ 13 â”€â”€â”¬â”€â”€ "iPhone 13" (priority=5.4)
                       |              â””â”€â”€ Pro â”€â”€â”¬â”€â”€ "iPhone 13 Pro" (priority=8.3)
                       |                        â””â”€â”€ Max â”€â”€ "iPhone 13 Pro Max" (priority=11.4)

BÆ°á»›c 3 - Prefix Matching:
- Query "iPhone 13 Pro Max" â†’ Ä‘i theo Ä‘Æ°á»ng dáº«n tree â†’ tÃ¬m tháº¥y "iPhone 13 Pro Max"
- Láº¥y top 50 candidates theo priority (trong trÆ°á»ng há»£p nÃ y chá»‰ láº¥y cÃ¡c node phÃ¹ há»£p)

BÆ°á»›c 4 - Cross-Encoder Reranking:
- So sÃ¡nh: "iPhone 13 Pro Max" vs "iPhone 13 Pro Max"
- Cross-encoder xÃ¡c nháº­n Ä‘á»™ tÆ°Æ¡ng Ä‘á»“ng cao nháº¥t â†’ Káº¿t quáº£: "iPhone 13 Pro Max"
```

### 2. Adaptive Priority + Semantic (`method_prefix_semantic_cross.py`)

**Approach**: Combines prefix tree matching with semantic search using dense embeddings

**Pipeline**:
1. Calculate adaptive priority for all categories
2. Build prefix tree from prioritized categories
3. Use prefix matching to find initial candidates
4. If no prefix match found, use semantic search
5. Apply cross-encoder reranking

**Use Case**: Effective when semantic meaning is important and categories may not share exact tokens with query

**Minh há»a cÃ¡ch hoáº¡t Ä‘á»™ng**:
```
Query: "Äiá»‡n thoáº¡i thÃ´ng minh Samsung Galaxy"
Categories: ["Mobile Devices", "Äiá»‡n thoáº¡i Android", "Äiá»‡n tá»­ tiÃªu dÃ¹ng", "Thiáº¿t bá»‹ di Ä‘á»™ng"]

BÆ°á»›c 1 - TÃ­nh Priority:
- "Mobile Devices": specificity=0.8, overlap=1, length=2 â†’ priority=2.8
- "Äiá»‡n thoáº¡i Android": specificity=0.9, overlap=1, length=2 â†’ priority=2.9
- "Äiá»‡n tá»­ tiÃªu dÃ¹ng": specificity=0.5, overlap=0, length=2 â†’ priority=1.5
- "Thiáº¿t bá»‹ di Ä‘á»™ng": specificity=0.6, overlap=0, length=2 â†’ priority=1.6

BÆ°á»›c 2 - Prefix Matching:
- Query khÃ´ng match vá»›i prefix tree (khÃ´ng cÃ³ node nÃ o báº¯t Ä‘áº§u vá»›i "Äiá»‡n thoáº¡i thÃ´ng minh...")

BÆ°á»›c 3 - Semantic Search (khi khÃ´ng cÃ³ prefix match):
- Encode query: "Äiá»‡n thoáº¡i thÃ´ng minh Samsung Galaxy" â†’ vector A
- Encode categories: ["Mobile Devices", "Äiá»‡n thoáº¡i Android", ...] â†’ vector [B, C, ...]
- TÃ­nh similarity: cos(A, B), cos(A, C), ...
- Gáº§n nháº¥t: "Äiá»‡n thoáº¡i Android" (vector C) - tÆ°Æ¡ng tá»± vá» ngá»¯ nghÄ©a

BÆ°á»›c 4 - Cross-Encoder Reranking:
- Pair: ["Äiá»‡n thoáº¡i thÃ´ng minh Samsung Galaxy", "Äiá»‡n thoáº¡i Android"]
- Cross-encoder xÃ¡c nháº­n má»‘i liÃªn káº¿t ngá»¯ nghÄ©a â†’ Káº¿t quáº£: "Äiá»‡n thoáº¡i Android"
```

### 3. Adaptive Priority + BM25 Lexical (`method_bm25_lexical_cross.py`)

**Approach**: Uses BM25 algorithm for more sophisticated lexical matching

**Pipeline**:
1. Calculate adaptive priority for all categories
2. Use BM25 algorithm for initial retrieval (top 50 candidates)
3. Apply cross-encoder reranking

**Use Case**: Better for scenarios with varied query lengths and token distributions

**Minh há»a cÃ¡ch hoáº¡t Ä‘á»™ng**:
```
Query: "Sá»¯a rá»­a máº·t táº¡o bá»t La Roche-Posay cho da nháº¡y cáº£m"
Categories: ["Sá»¯a rá»­a máº·t", "ChÄƒm sÃ³c da", "Má»¹ pháº©m", "Sá»¯a rá»­a máº·t cho da nháº¡y cáº£m", "La Roche-Posay"]

BÆ°á»›c 1 - TÃ­nh Priority:
- "Sá»¯a rá»­a máº·t": specificity=0.3, overlap=2, length=3 â†’ priority=5.3
- "ChÄƒm sÃ³c da": specificity=0.4, overlap=0, length=2 â†’ priority=1.4
- "Má»¹ pháº©m": specificity=0.2, overlap=0, length=1 â†’ priority=0.2
- "Sá»¯a rá»­a máº·t cho da nháº¡y cáº£m": specificity=0.7, overlap=5, length=5 â†’ priority=12.9
- "La Roche-Posay": specificity=0.9, overlap=1, length=2 â†’ priority=3.9

BÆ°á»›c 2 - BM25 Retrieval:
- TÃ­nh BM25 score cho tá»«ng category:
  - "Sá»¯a rá»­a máº·t": TF-IDF based score = 0.4
  - "ChÄƒm sÃ³c da": TF-IDF based score = 0.1
  - "Má»¹ pháº©m": TF-IDF based score = 0.05
  - "Sá»¯a rá»­a máº·t cho da nháº¡y cáº£m": TF-IDF based score = 1.2
  - "La Roche-Posay": TF-IDF based score = 0.8

- Sáº¯p xáº¿p: ["Sá»¯a rá»­a máº·t cho da nháº¡y cáº£m", "La Roche-Posay", "Sá»¯a rá»­a máº·t", ...]
- Láº¥y top 50 candidates (trong trÆ°á»ng há»£p nÃ y lÃ  táº¥t cáº£)

BÆ°á»›c 3 - Cross-Encoder Reranking:
- So sÃ¡nh tá»«ng cáº·p vá»›i cross-encoder
- ["Sá»¯a rá»­a máº·t táº¡o bá»t La Roche-Posay cho da nháº¡y cáº£m", "Sá»¯a rá»­a máº·t cho da nháº¡y cáº£m"] â†’ score cao nháº¥t
- Káº¿t quáº£: "Sá»¯a rá»­a máº·t cho da nháº¡y cáº£m" (token overlap: 5, Ä‘á»™ phÃ¹ há»£p cao)
```

### 4. Optimized Pipeline + BM25 (`method_bm25_lexical_cross.py`)

**Approach**: Enhanced version of BM25 method with optimized retrieval

**Pipeline**:
1. Create prioritized products with auto priority calculation
2. Initialize BM25Retriever with prioritized products
3. Retrieve top 50 candidates using optimized BM25
4. Cross-encoder reranking for final ranking

**Use Case**: When you need the most optimized lexical approach

**Minh há»a cÃ¡ch hoáº¡t Ä‘á»™ng**:
```
Query: "MacBook Pro 16 inch 2022 M1 Pro"
Categories: ["Laptop", "MacBook", "MacBook Pro", "MacBook Pro 16 inch", "MacBook Pro 16 inch 2022", "MacBook Pro 16 inch 2022 M1 Pro", "MÃ¡y tÃ­nh"]

BÆ°á»›c 1 - Táº¡o prioritized products:
- TÃ­nh priority cho tá»«ng category dá»±a trÃªn query
- ["MacBook Pro 16 inch 2022 M1 Pro": priority=15.2, "MacBook Pro 16 inch 2022": priority=13.1, ...]

BÆ°á»›c 2 - Initialize BM25Retriever:
- Index cÃ¡c categories Ä‘Ã£ Ä‘Æ°á»£c sáº¯p xáº¿p theo priority
- BM25Retriever([prioritized_categories])

BÆ°á»›c 3 - BM25 Search:
- Query: "MacBook Pro 16 inch 2022 M1 Pro"
- BM25 tÃ¬m top 50 candidates, xáº¿p theo relevance score
- Káº¿t quáº£: ["MacBook Pro 16 inch 2022 M1 Pro", "MacBook Pro 16 inch 2022", "MacBook Pro 16 inch", ...]

BÆ°á»›c 4 - Cross-Encoder Reranking:
- TÃ­nh cross-encoder score cho tá»«ng cáº·p query-category
- ["MacBook Pro 16 inch 2022 M1 Pro", "MacBook Pro 16 inch 2022 M1 Pro"] â†’ score cao nháº¥t
- Káº¿t quáº£: "MacBook Pro 16 inch 2022 M1 Pro"
```

### 5. Inverted Index (`method_scalable_inverted_ngram_hybrid.py`)

**Approach**: Scalable method using inverted index for efficient retrieval

**Pipeline**:
1. Build inverted index mapping tokens to category indices
2. For query, find intersection of token sets to get candidates
3. Cross-encoder reranking

**Time Complexity**: O(Q Ã— K + C Ã— log C) where:
- Q: number of tokens in query
- K: average categories per token (~10-100)
- C: number of candidates (~10-100)

**Use Case**: Essential for large-scale datasets with 100K+ categories

**Minh há»a cÃ¡ch hoáº¡t Ä‘á»™ng**:
```
Query: "sá»¯a rá»­a máº·t"
Categories: [
  "Äiá»‡n thoáº¡i iPhone",          # index: 0
  "Sá»¯a rá»­a máº·t La Roche-Posay", # index: 1
  "Sá»¯a táº¯m gá»™i",               # index: 2
  "Má»¹ pháº©m chÄƒm sÃ³c da",       # index: 3
  "Sá»¯a rá»­a máº·t táº¡o bá»t"        # index: 4
]

BÆ°á»›c 1 - Build Inverted Index:
- "Ä‘iá»‡n": {0}
- "thoáº¡i": {0}
- "iphone": {0}
- "sá»¯a": {1, 2, 4}     # cÃ¡c categories chá»©a token "sá»¯a"
- "rá»­a": {1, 4}        # cÃ¡c categories chá»©a token "rá»­a"
- "máº·t": {1, 4}        # cÃ¡c categories chá»©a token "máº·t"
- "la": {1}
- "roche": {1}
- "posay": {1}
- "táº¯m": {2}
- "gá»™i": {2}
- "má»¹": {3}
- "pháº©m": {3}
- "chÄƒm": {3}
- "sÃ³c": {3}
- "da": {3}
- "táº¡o": {4}
- "bá»t": {4}

BÆ°á»›c 2 - Query Processing:
- Query: "sá»¯a rá»­a máº·t" â†’ tokens = {"sá»¯a", "rá»­a", "máº·t"}
- TÃ¬m intersection: tokens["sá»¯a"] âˆ© tokens["rá»­a"] âˆ© tokens["máº·t"]
- = {1, 2, 4} âˆ© {1, 4} âˆ© {1, 4} = {1, 4}

BÆ°á»›c 3 - Láº¥y candidates:
- CÃ¡c categories phÃ¹ há»£p: ["Sá»¯a rá»­a máº·t La Roche-Posay" (index: 1), "Sá»¯a rá»­a máº·t táº¡o bá»t" (index: 4)]

BÆ°á»›c 4 - Cross-Encoder Reranking:
- So sÃ¡nh: ["sá»¯a rá»­a máº·t", "Sá»¯a rá»­a máº·t La Roche-Posay"] vÃ  ["sá»¯a rá»­a máº·t", "Sá»¯a rá»­a máº·t táº¡o bá»t"]
- Cross-encoder chá»n á»©ng viÃªn phÃ¹ há»£p nháº¥t
- Káº¿t quáº£: "Sá»¯a rá»­a máº·t táº¡o bá»t" (gáº§n vá»›i query hÆ¡n)
```

### 6. N-gram Index (`method_scalable_inverted_ngram_hybrid.py`)

**Approach**: Scalable method using n-gram matching for better partial matching

**Pipeline**:
1. Build n-gram index from all categories (unigrams, bigrams, trigrams)
2. Generate n-grams from query
3. Find matches and score candidates
4. Cross-encoder reranking

**Use Case**: When dealing with partial matches and typos in queries

**Minh há»a cÃ¡ch hoáº¡t Ä‘á»™ng**:
```
Query: "iphone 13"
Categories: [
  "Äiá»‡n thoáº¡i iPhone 13 Pro",  # index: 0
  "iPhone 13",                # index: 1
  "Phá»¥ kiá»‡n iPhone",          # index: 2
  "Samsung Galaxy",           # index: 3
  "iPhone 13 Pro Max"         # index: 4
]

BÆ°á»›c 1 - Build N-gram Index:
Äiá»‡n thoáº¡i iPhone 13 Pro (index 0):
  - unigrams: {"Ä‘iá»‡n", "thoáº¡i", "iphone", "13", "pro"}
  - bigrams: {"Ä‘iá»‡n thoáº¡i", "thoáº¡i iphone", "iphone 13", "13 pro"}
  - trigrams: {"Ä‘iá»‡n thoáº¡i iphone", "thoáº¡i iphone 13", "iphone 13 pro"}
  â†’ N-gram index: {"Ä‘iá»‡n": {0}, "thoáº¡i": {0}, "iphone": {0,1,2}, "13": {0,1,4}, "pro": {0,4}, "Ä‘iá»‡n thoáº¡i": {0}, "thoáº¡i iphone": {0}, "iphone 13": {0}, "13 pro": {0}, ...}

iPhone 13 (index 1):
  - unigrams: {"iphone", "13"}
  - bigrams: {"iphone 13"}
  - trigrams: {} (khÃ´ng Ä‘á»§ 3 token)
  â†’ ThÃªm vÃ o index: {"iphone": {0,1,2}, "13": {0,1,4}, "iphone 13": {1}, ...}

Phá»¥ kiá»‡n iPhone (index 2):
  - unigrams: {"phá»¥", "kiá»‡n", "iphone"}
  - bigrams: {"phá»¥ kiá»‡n", "kiá»‡n iphone"}
  - trigrams: {}
  â†’ ThÃªm vÃ o index: {"phá»¥": {2}, "kiá»‡n": {2}, "iphone": {0,1,2}, "phá»¥ kiá»‡n": {2}, "kiá»‡n iphone": {2}, ...}

BÆ°á»›c 2 - Query N-gram Generation:
- Query: "iphone 13" â†’ tokens = ["iphone", "13"]
- Unigrams: ["iphone", "13"] â†’ scores: [1.0, 1.0]
- Bigrams: ["iphone 13"] â†’ score: [2.0] (Æ°u tiÃªn bigram dÃ i hÆ¡n)

BÆ°á»›c 3 - TÃ¬m candidates tá»« n-gram index:
- "iphone" â†’ {0, 1, 2} (Äiá»‡n thoáº¡i iPhone 13 Pro, iPhone 13, Phá»¥ kiá»‡n iPhone)
- "13" â†’ {0, 1, 4} (Äiá»‡n thoáº¡i iPhone 13 Pro, iPhone 13, iPhone 13 Pro Max)
- "iphone 13" â†’ {0, 1} (Äiá»‡n thoáº¡i iPhone 13 Pro, iPhone 13)
- Gá»™p vÃ  tÃ­nh score:
  - Category 0 ("Äiá»‡n thoáº¡i iPhone 13 Pro"): score = 2.0 (bigram) + 1.0 (iphone) + 1.0 (13) = 4.0
  - Category 1 ("iPhone 13"): score = 2.0 (bigram) + 1.0 (iphone) + 1.0 (13) = 4.0
  - Category 2 ("Phá»¥ kiá»‡n iPhone"): score = 1.0 (iphone) = 1.0
  - Category 4 ("iPhone 13 Pro Max"): score = 1.0 (13) + 1.0 (iphone) = 2.0

BÆ°á»›c 4 - Cross-Encoder Reranking:
- So sÃ¡nh cÃ¡c candidates vá»›i query qua cross-encoder
- ["iphone 13", "iPhone 13"] â†’ score cao nháº¥t do match chÃ­nh xÃ¡c
- Káº¿t quáº£: "iPhone 13"
```

### 7. Hybrid (`method_scalable_inverted_ngram_hybrid.py`)

**Approach**: Combines inverted index (lexical) with semantic search

**Pipeline**:
1. Inverted index retrieval â†’ ~20-30 lexical candidates
2. Semantic search â†’ ~20-30 semantic candidates
3. Union to get ~40-60 unique candidates
4. Cross-encoder reranking

**Use Case**: Best of both worlds - handles exact matches and semantic similarity

**Minh há»a cÃ¡ch hoáº¡t Ä‘á»™ng**:
```
Query: "smartphone"
Categories: [
  "Äiá»‡n thoáº¡i thÃ´ng minh",      # index: 0
  "Mobile",                      # index: 1
  "iPhone",                      # index: 2
  "Samsung",                     # index: 3
  "MÃ¡y tÃ­nh báº£ng",              # index: 4
  "Laptop",                      # index: 5
  "Äiá»‡n thoáº¡i di Ä‘á»™ng"          # index: 6
]

BÆ°á»›c 1 - Inverted Index Retrieval:
- Query "smartphone" â†’ normalize â†’ tokens = {"smartphone"} (khÃ´ng match trá»±c tiáº¿p)
- KhÃ´ng cÃ³ match chÃ­nh xÃ¡c, fallback: Union cá»§a cÃ¡c token gáº§n giá»‘ng
- Giáº£ sá»­ há»‡ thá»‘ng cÃ³ fuzzy matching tÃ¬m Ä‘Æ°á»£c: {"phone", "mobile"} (náº¿u cÃ³)
- Hoáº·c náº¿u query Ä‘Æ°á»£c dá»‹ch: "smartphone" â†’ "Ä‘iá»‡n thoáº¡i thÃ´ng minh" â†’ tokens = {"Ä‘iá»‡n", "thoáº¡i", "thÃ´ng", "minh"}
  â†’ Match vá»›i cÃ¡c category chá»©a cÃ¡c tá»« nÃ y
- Lexical candidates: ["Äiá»‡n thoáº¡i thÃ´ng minh" (0), "Mobile" (1), "Äiá»‡n thoáº¡i di Ä‘á»™ng" (6)]

BÆ°á»›c 2 - Semantic Retrieval:
- Encode query "smartphone" thÃ nh vector A
- Encode táº¥t cáº£ categories thÃ nh vectors [B0, B1, B2, B3, B4, B5, B6]
- TÃ­nh cosine similarity: cos(A, Bi) vá»›i i=0..6
- Top semantic matches (giáº£ sá»­):
  - "Äiá»‡n thoáº¡i thÃ´ng minh" (sim=0.9) - index: 0
  - "Äiá»‡n thoáº¡i di Ä‘á»™ng" (sim=0.85) - index: 6
  - "Mobile" (sim=0.78) - index: 1
  - "iPhone" (sim=0.65) - index: 2

BÆ°á»›c 3 - Union Candidates:
- Lexical: {0, 1, 6} ("Äiá»‡n thoáº¡i thÃ´ng minh", "Mobile", "Äiá»‡n thoáº¡i di Ä‘á»™ng")
- Semantic: {0, 6, 1, 2} (top 4)
- Union: {0, 1, 6, 2} = ["Äiá»‡n thoáº¡i thÃ´ng minh", "Mobile", "Äiá»‡n thoáº¡i di Ä‘á»™ng", "iPhone"]

BÆ°á»›c 4 - Cross-Encoder Reranking:
- So sÃ¡nh tá»«ng cáº·p query-candidate:
  - ["smartphone", "Äiá»‡n thoáº¡i thÃ´ng minh"] â†’ score=0.95
  - ["smartphone", "Mobile"] â†’ score=0.87
  - ["smartphone", "Äiá»‡n thoáº¡i di Ä‘á»™ng"] â†’ score=0.83
  - ["smartphone", "iPhone"] â†’ score=0.75
- Top score: "Äiá»‡n thoáº¡i thÃ´ng minh"
- Káº¿t quáº£: "Äiá»‡n thoáº¡i thÃ´ng minh" (match cáº£ lexical vÃ  semantic)
```

## Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Query Input   â”‚â”€â”€â”€â–¶â”‚  Method Selector â”‚â”€â”€â”€â–¶â”‚  Method Pipelineâ”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                          â”‚
           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Priority Calc.  â”‚â”€â”€â”€â–¶â”‚  Retrieval       â”‚â”€â”€â”€â–¶â”‚ Cross-Encoder   â”‚
â”‚ (Specificity,   â”‚    â”‚  (BM25, Prefix,  â”‚    â”‚  Reranking      â”‚
â”‚  Overlap, Len)  â”‚    â”‚   Inverted, etc) â”‚    â”‚                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Key Components

- **Priority Calculator**: Calculates composite scores based on specificity (IDF-based), token overlap with query, and category length
- **Retrieval Models**: Different algorithms for finding candidate categories
- **Cross-Encoder**: Final reranking using semantic similarity between query and category
- **Scalable Indexes**: Optimized data structures for handling 100K+ categories

## Usage Examples

### Single Product Search

```python
from methods.method_bm25_lexical_cross import method_optimized_pipeline
from models.cross_encoder import Qwen3CrossEncoder

# Initialize models
cross_encoder = Qwen3CrossEncoder()

query = "iPhone 13 Pro Max 256GB"
categories = [
    "Äiá»‡n thoáº¡i",
    "iPhone", 
    "iPhone 13",
    "iPhone 13 Pro Max",
    "Äiá»‡n tá»­ tiÃªu dÃ¹ng",
    "Thiáº¿t bá»‹ di Ä‘á»™ng"
]

result, score, method_used, debug_info = method_optimized_pipeline(
    query=query,
    categories=categories,
    cross_encoder=cross_encoder,
    semantic_encoder=None,
    idx=0
)

print(f"Best match: {result}")
print(f"Confidence score: {score:.3f}")
print(f"Method used: {method_used}")
print(f"Debug info: {debug_info}")
```

### Batch Evaluation

The system also includes a batch evaluation feature that processes CSV files:

```python
# CSV format:
# tÃªn hÃ ng hÃ³a,káº¿t quáº£ mong muá»‘n,danh má»¥c
# iPhone 13 Pro Max,Äiá»‡n thoáº¡i iPhone,"Äiá»‡n thoáº¡i,Äiá»‡n tá»­,iPhone,iPhone 13,iPhone 13 Pro Max,Äiá»‡n tá»­ tiÃªu dÃ¹ng"
```

## Performance Comparison

| Method | Accuracy | Use Case | Complexity |
|--------|----------|----------|------------|
| Adaptive Lexical | Good | Small datasets, prefix matching | O(n) |
| Adaptive Semantic | Good | Semantic similarity important | O(nÃ—d) |
| BM25 Lexical | Very Good | Varied query lengths | O(n) |
| Optimized BM25 | Excellent | Best lexical approach | O(n) |
| Inverted Index | Good | 100K+ categories | O(1) |
| N-gram Index | Good | Partial matching | O(1) |
| Hybrid | Best | Best overall performance | O(1) + O(nÃ—d) |

Where n is the number of categories and d is the embedding dimension.

## Installation

1. Clone the repository
2. Install dependencies (see requirements if available)
3. Run the application:

```bash
python app.py
```

The application provides both a single search interface and batch evaluation capabilities.