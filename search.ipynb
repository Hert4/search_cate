{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84ee31b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "# from typing import List, Tuple, Optional\n",
    "# import re\n",
    "# from dataclasses import dataclass\n",
    "# import torch\n",
    "# import torch.nn.functional as F\n",
    "# from transformers import AutoTokenizer, AutoModel, AutoModelForCausalLM\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# from tqdm import tqdm\n",
    "\n",
    "# @dataclass\n",
    "# class Product:\n",
    "#     id: str\n",
    "#     name: str\n",
    "#     category_path: List[str]\n",
    "\n",
    "\n",
    "# class PrefixTreeNode:\n",
    "#     def __init__(self, token: str):\n",
    "#         self.token = token\n",
    "#         self.children = {}\n",
    "#         self.products = []\n",
    "    \n",
    "#     def add_child(self, token: str):\n",
    "#         if token not in self.children:\n",
    "#             self.children[token] = PrefixTreeNode(token)\n",
    "#         return self.children[token]\n",
    "\n",
    "\n",
    "# class SemanticEncoder:\n",
    "#     \"\"\"Encoder ƒë·ªÉ t·∫°o embeddings cho semantic search\"\"\"\n",
    "#     def __init__(self, model_name: str = \"Qwen/Qwen3-Embedding-0.6B\", device: str = None):\n",
    "#         print(\"Loading Semantic Encoder...\")\n",
    "#         if device is None:\n",
    "#             device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        \n",
    "#         self.device = device\n",
    "#         self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "#         self.model = AutoModel.from_pretrained(model_name).to(device).eval()\n",
    "#         print(f\"‚úÖ Semantic Encoder loaded on {device}\")\n",
    "    \n",
    "#     def encode(self, texts: List[str]) -> np.ndarray:\n",
    "#         \"\"\"Encode texts th√†nh embeddings\"\"\"\n",
    "#         with torch.no_grad():\n",
    "#             inputs = self.tokenizer(texts, padding=True, truncation=True, \n",
    "#                                   return_tensors=\"pt\", max_length=512)\n",
    "#             inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
    "            \n",
    "#             outputs = self.model(**inputs)\n",
    "#             embeddings = outputs.last_hidden_state.mean(dim=1)\n",
    "#             embeddings = F.normalize(embeddings, p=2, dim=1)\n",
    "            \n",
    "#         return embeddings.cpu().numpy()\n",
    "\n",
    "\n",
    "# class Qwen3CrossEncoder:\n",
    "#     def __init__(self, model_name: str = \"Qwen/Qwen3-Reranker-0.6B\", device: str = None):\n",
    "#         print(\"oading Qwen3-Reranker...\")\n",
    "#         if device is None:\n",
    "#             device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        \n",
    "#         self.device = device\n",
    "#         self.tokenizer = AutoTokenizer.from_pretrained(model_name, padding_side='left')\n",
    "#         self.model = AutoModelForCausalLM.from_pretrained(model_name).to(device).eval()\n",
    "        \n",
    "#         self.token_false_id = self.tokenizer.convert_tokens_to_ids(\"no\")\n",
    "#         self.token_true_id = self.tokenizer.convert_tokens_to_ids(\"yes\")\n",
    "#         self.max_length = 8192\n",
    "        \n",
    "#         self.prefix = \"<|im_start|>system\\nJudge whether the Document meets the requirements based on the Query and the Instruct provided. Note that the answer can only be \\\"yes\\\" or \\\"no\\\".<|im_end|>\\n<|im_start|>user\\n\"\n",
    "#         self.suffix = \"<|im_end|>\\n<|im_start|>assistant\\n<think>\\n\\n</think>\\n\\n\"\n",
    "#         self.prefix_tokens = self.tokenizer.encode(self.prefix, add_special_tokens=False)\n",
    "#         self.suffix_tokens = self.tokenizer.encode(self.suffix, add_special_tokens=False)\n",
    "        \n",
    "#         self.task_instruction = \"Given a product search query, judge whether the product matches the user's search intent\"\n",
    "#         print(f\"‚úÖ Cross-encoder loaded on {device}\")\n",
    "    \n",
    "#     def format_instruction(self, query: str, doc: str) -> str:\n",
    "#         return \"<Instruct>: {instruction}\\n<Query>: {query}\\n<Document>: {doc}\".format(\n",
    "#             instruction=self.task_instruction, query=query, doc=doc\n",
    "#         )\n",
    "    \n",
    "#     def process_inputs(self, pairs: List[str]):\n",
    "#         inputs = self.tokenizer(\n",
    "#             pairs, padding=False, truncation='longest_first',\n",
    "#             return_attention_mask=False, \n",
    "#             max_length=self.max_length - len(self.prefix_tokens) - len(self.suffix_tokens)\n",
    "#         )\n",
    "        \n",
    "#         for i, ele in enumerate(inputs['input_ids']):\n",
    "#             inputs['input_ids'][i] = self.prefix_tokens + ele + self.suffix_tokens\n",
    "        \n",
    "#         inputs = self.tokenizer.pad(inputs, padding=True, return_tensors=\"pt\", max_length=self.max_length)\n",
    "        \n",
    "#         for key in inputs:\n",
    "#             inputs[key] = inputs[key].to(self.device)\n",
    "        \n",
    "#         return inputs\n",
    "    \n",
    "#     def compute_logits(self, inputs):\n",
    "#         with torch.no_grad():\n",
    "#             batch_scores = self.model(**inputs).logits[:, -1, :]\n",
    "#             true_vector = batch_scores[:, self.token_true_id]\n",
    "#             false_vector = batch_scores[:, self.token_false_id]\n",
    "#             batch_scores = torch.stack([false_vector, true_vector], dim=1)\n",
    "#             batch_scores = torch.nn.functional.log_softmax(batch_scores, dim=1)\n",
    "#             scores = batch_scores[:, 1].exp().tolist()\n",
    "#         return scores\n",
    "    \n",
    "#     def predict(self, pairs: List[List[str]]) -> List[float]:\n",
    "#         formatted_pairs = [self.format_instruction(query, doc) for query, doc in pairs]\n",
    "#         inputs = self.process_inputs(formatted_pairs)\n",
    "#         scores = self.compute_logits(inputs)\n",
    "#         return scores\n",
    "\n",
    "\n",
    "# def parse_categories(category_str: str) -> List[str]:\n",
    "#     \"\"\"Parse danh m·ª•c t·ª´ string\"\"\"\n",
    "#     category_str = category_str.strip()\n",
    "#     if category_str.startswith('[') and category_str.endswith(']'):\n",
    "#         category_str = category_str[1:-1]\n",
    "#     categories = [cat.strip() for cat in category_str.split(',')]\n",
    "#     return categories\n",
    "\n",
    "\n",
    "# def normalize_text(text: str) -> List[str]:\n",
    "#     \"\"\"Chu·∫©n h√≥a v√† t√°ch t·ª´\"\"\"\n",
    "#     text = text.lower()\n",
    "#     tokens = re.findall(r'\\w+', text)\n",
    "#     return tokens\n",
    "\n",
    "\n",
    "# def build_prefix_tree(products: List[Product]) -> PrefixTreeNode:\n",
    "#     \"\"\"X√¢y d·ª±ng prefix tree\"\"\"\n",
    "#     root = PrefixTreeNode(\"ROOT\")\n",
    "    \n",
    "#     for product in products:\n",
    "#         current = root\n",
    "#         full_path = \" \".join(product.category_path)\n",
    "#         tokens = normalize_text(full_path)\n",
    "        \n",
    "#         # Lo·∫°i b·ªè duplicate li√™n ti·∫øp\n",
    "#         unique_tokens = []\n",
    "#         for token in tokens:\n",
    "#             if not unique_tokens or token != unique_tokens[-1]:\n",
    "#                 unique_tokens.append(token)\n",
    "        \n",
    "#         for token in unique_tokens:\n",
    "#             current = current.add_child(token)\n",
    "        \n",
    "#         current.products.append(product)\n",
    "    \n",
    "#     return root\n",
    "\n",
    "\n",
    "# def find_matching_node(tree: PrefixTreeNode, query: str) -> Optional[PrefixTreeNode]:\n",
    "#     \"\"\"T√¨m node s√¢u nh·∫•t match v·ªõi query\"\"\"\n",
    "#     query_tokens = normalize_text(query)\n",
    "    \n",
    "#     current = tree\n",
    "#     deepest_with_products = None\n",
    "    \n",
    "#     for token in query_tokens:\n",
    "#         if token in current.children:\n",
    "#             current = current.children[token]\n",
    "#             if current.products:\n",
    "#                 deepest_with_products = current\n",
    "#         else:\n",
    "#             break\n",
    "    \n",
    "#     return deepest_with_products\n",
    "\n",
    "\n",
    "# def lexical_search(query: str, products: List[Product], top_n: int = 50) -> List[Product]:\n",
    "#     \"\"\"Lexical search d·ª±a tr√™n token matching\"\"\"\n",
    "#     query_tokens = normalize_text(query)\n",
    "    \n",
    "#     scores = []\n",
    "#     for product in products:\n",
    "#         all_text = product.name + \" \" + \" \".join(product.category_path)\n",
    "#         product_tokens = normalize_text(all_text)\n",
    "        \n",
    "#         matches = sum(1 for qt in query_tokens if qt in product_tokens)\n",
    "#         score = matches / len(query_tokens) if query_tokens else 0\n",
    "        \n",
    "#         scores.append((product, score))\n",
    "    \n",
    "#     scores.sort(key=lambda x: x[1], reverse=True)\n",
    "#     return [p for p, s in scores[:top_n] if s > 0]\n",
    "\n",
    "\n",
    "# def semantic_search(query: str, products: List[Product], \n",
    "#                     semantic_encoder: SemanticEncoder, top_n: int = 50) -> List[Product]:\n",
    "#     \"\"\"Semantic search d·ª±a tr√™n embeddings\"\"\"\n",
    "#     query_emb = semantic_encoder.encode([query])[0]\n",
    "    \n",
    "#     texts = [p.name for p in products]\n",
    "#     product_embs = semantic_encoder.encode(texts)\n",
    "    \n",
    "#     similarities = [float(np.dot(query_emb, emb)) for emb in product_embs]\n",
    "    \n",
    "#     scored = list(zip(products, similarities))\n",
    "#     scored.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "#     return [p for p, s in scored[:top_n]]\n",
    "\n",
    "\n",
    "# def method_prefix_lexical_cross(query: str, categories: List[str],\n",
    "#                                  cross_encoder: Qwen3CrossEncoder,\n",
    "#                                  idx: int) -> Tuple[str, float, str]:\n",
    "#     \"\"\"Ph∆∞∆°ng ph√°p 1: Prefix Tree ‚Üí Lexical Search ‚Üí Cross-encoder\"\"\"\n",
    "#     # T·∫°o products\n",
    "#     products = [Product(f\"{idx}_{i}\", cat, [cat]) for i, cat in enumerate(categories)]\n",
    "    \n",
    "#     # Build prefix tree\n",
    "#     tree = build_prefix_tree(products)\n",
    "    \n",
    "#     # B∆∞·ªõc 1: Prefix matching\n",
    "#     matched_node = find_matching_node(tree, query)\n",
    "    \n",
    "#     if matched_node and matched_node.products:\n",
    "#         candidates = matched_node.products\n",
    "#         retrieval_method = \"prefix\"\n",
    "#     else:\n",
    "#         # B∆∞·ªõc 2: Lexical search (fallback)\n",
    "#         candidates = lexical_search(query, products, top_n=50)\n",
    "#         retrieval_method = \"lexical\"\n",
    "    \n",
    "#     if not candidates:\n",
    "#         return categories[0], 0.0, \"no_candidates\"\n",
    "    \n",
    "#     # B∆∞·ªõc 3: Cross-encoder\n",
    "#     pairs = [[query, p.name] for p in candidates]\n",
    "#     scores = cross_encoder.predict(pairs)\n",
    "    \n",
    "#     best_idx = np.argmax(scores)\n",
    "#     return candidates[best_idx].name, scores[best_idx], retrieval_method\n",
    "\n",
    "\n",
    "# def method_prefix_semantic_cross(query: str, categories: List[str],\n",
    "#                                   semantic_encoder: SemanticEncoder,\n",
    "#                                   cross_encoder: Qwen3CrossEncoder,\n",
    "#                                   idx: int) -> Tuple[str, float, str]:\n",
    "#     \"\"\"Ph∆∞∆°ng ph√°p 2: Prefix Tree ‚Üí Semantic Search ‚Üí Cross-encoder (M·ªöI)\"\"\"\n",
    "#     # T·∫°o products\n",
    "#     products = [Product(f\"{idx}_{i}\", cat, [cat]) for i, cat in enumerate(categories)]\n",
    "    \n",
    "#     # Build prefix tree\n",
    "#     tree = build_prefix_tree(products)\n",
    "    \n",
    "#     # B∆∞·ªõc 1: Prefix matching\n",
    "#     matched_node = find_matching_node(tree, query)\n",
    "    \n",
    "#     if matched_node and matched_node.products:\n",
    "#         candidates = matched_node.products\n",
    "#         retrieval_method = \"prefix\"\n",
    "#     else:\n",
    "#         # B∆∞·ªõc 2: Semantic search (fallback) - THAY ƒê·ªîI CH·ªñ N√ÄY\n",
    "#         candidates = semantic_search(query, products, semantic_encoder, top_n=50)\n",
    "#         retrieval_method = \"semantic\"\n",
    "    \n",
    "#     if not candidates:\n",
    "#         return categories[0], 0.0, \"no_candidates\"\n",
    "    \n",
    "#     # B∆∞·ªõc 3: Cross-encoder\n",
    "#     pairs = [[query, p.name] for p in candidates]\n",
    "#     scores = cross_encoder.predict(pairs)\n",
    "    \n",
    "#     best_idx = np.argmax(scores)\n",
    "#     return candidates[best_idx].name, scores[best_idx], retrieval_method\n",
    "\n",
    "\n",
    "# def compare_all_methods(csv_path: str, device: str = \"cpu\", verbose: bool = False):\n",
    "#     \"\"\"\n",
    "#     So s√°nh 2 ph∆∞∆°ng ph√°p:\n",
    "#     1. Prefix ‚Üí Lexical ‚Üí Cross \n",
    "#     2. Prefix ‚Üí Semantic ‚Üí Cross \n",
    "#     \"\"\"\n",
    "#     print(\"=\" * 80)\n",
    "#     print(\"COMPARING: LEXICAL vs SEMANTIC SEARCH (in Prefix+Retrieval+Cross pipeline)\")\n",
    "#     print(\"=\" * 80)\n",
    "    \n",
    "#     # Load data\n",
    "#     df = pd.read_csv(csv_path)\n",
    "#     print(f\"\\n‚úÖ Loaded {len(df)} test cases\")\n",
    "    \n",
    "#     # Initialize models\n",
    "#     print(\"\\n\" + \"=\" * 80)\n",
    "#     print(\"INITIALIZING MODELS\")\n",
    "#     print(\"=\" * 80)\n",
    "    \n",
    "#     semantic_encoder = SemanticEncoder(device=device)\n",
    "#     cross_encoder = Qwen3CrossEncoder(device=device)\n",
    "    \n",
    "#     # Results storage\n",
    "#     results = []\n",
    "    \n",
    "#     print(\"\\n\" + \"=\" * 80)\n",
    "#     print(\"RUNNING TESTS\")\n",
    "#     print(\"=\" * 80)\n",
    "    \n",
    "#     # Progress bar\n",
    "#     for idx, row in tqdm(df.iterrows(), total=len(df), desc=\"Processing\"):\n",
    "#         query = row['t√™n h√†ng h√≥a c·∫ßn map']\n",
    "#         expected = row['k·∫øt qu·∫£ mong mu·ªën']\n",
    "#         categories_str = row['danh m·ª•c']\n",
    "#         categories = parse_categories(categories_str)\n",
    "        \n",
    "#         if verbose:\n",
    "#             print(f\"\\n--- Test {idx + 1}/{len(df)} ---\")\n",
    "#             print(f\"Query: {query}\")\n",
    "#             print(f\"Expected: {expected}\")\n",
    "        \n",
    "#         # METHOD 1: Prefix ‚Üí Lexical ‚Üí Cross\n",
    "#         pred1, score1, retrieval1 = method_prefix_lexical_cross(\n",
    "#             query, categories, cross_encoder, idx\n",
    "#         )\n",
    "#         correct1 = (pred1.strip().lower() == expected.strip().lower())\n",
    "        \n",
    "#         # METHOD 2: Prefix ‚Üí Semantic ‚Üí Cross\n",
    "#         pred2, score2, retrieval2 = method_prefix_semantic_cross(\n",
    "#             query, categories, semantic_encoder, cross_encoder, idx\n",
    "#         )\n",
    "#         correct2 = (pred2.strip().lower() == expected.strip().lower())\n",
    "        \n",
    "#         if verbose:\n",
    "#             print(f\"  Method 1 (Prefix‚ÜíLexical‚ÜíCross):  {pred1} [{score1:.3f}] via {retrieval1} {'‚úÖ' if correct1 else '‚ùå'}\")\n",
    "#             print(f\"  Method 2 (Prefix‚ÜíSemantic‚ÜíCross): {pred2} [{score2:.3f}] via {retrieval2} {'‚úÖ' if correct2 else '‚ùå'}\")\n",
    "        \n",
    "#         results.append({\n",
    "#             'query': query,\n",
    "#             'categories': str(categories),\n",
    "#             'expected': expected,\n",
    "            \n",
    "#             # Method 1: Prefix + Lexical + Cross\n",
    "#             'lexical_pred': pred1,\n",
    "#             'lexical_score': score1,\n",
    "#             'lexical_retrieval': retrieval1,\n",
    "#             'lexical_correct': correct1,\n",
    "            \n",
    "#             # Method 2: Prefix + Semantic + Cross\n",
    "#             'semantic_pred': pred2,\n",
    "#             'semantic_score': score2,\n",
    "#             'semantic_retrieval': retrieval2,\n",
    "#             'semantic_correct': correct2,\n",
    "#         })\n",
    "    \n",
    "#     # Convert to DataFrame\n",
    "#     results_df = pd.DataFrame(results)\n",
    "    \n",
    "#     # Calculate accuracies\n",
    "#     acc1 = (results_df['lexical_correct'].sum() / len(results_df)) * 100\n",
    "#     acc2 = (results_df['semantic_correct'].sum() / len(results_df)) * 100\n",
    "    \n",
    "#     # Print summary\n",
    "#     print(\"\\n\" + \"=\" * 80)\n",
    "#     print(\"FINAL RESULTS\")\n",
    "#     print(\"=\" * 80)\n",
    "    \n",
    "#     print(f\"\\n{'Method':<50} {'Accuracy':<12} {'Correct/Total'}\")\n",
    "#     print(\"-\" * 80)\n",
    "#     print(f\"{'1. Prefix ‚Üí Lexical ‚Üí Cross-encoder':<50} {acc1:>6.2f}%     {results_df['lexical_correct'].sum():>3}/{len(results_df)}\")\n",
    "#     print(f\"{'2. Prefix ‚Üí Semantic ‚Üí Cross-encoder':<50} {acc2:>6.2f}%     {results_df['semantic_correct'].sum():>3}/{len(results_df)}\")\n",
    "#     print(\"-\" * 80)\n",
    "    \n",
    "#     improvement = acc2 - acc1\n",
    "#     print(f\"\\nüìä Semantic over Lexical: {improvement:>+6.2f}%\")\n",
    "    \n",
    "#     # Retrieval method breakdown\n",
    "#     print(\"\\n\" + \"=\" * 80)\n",
    "#     print(\"RETRIEVAL METHOD BREAKDOWN\")\n",
    "#     print(\"=\" * 80)\n",
    "    \n",
    "#     print(\"\\nMethod 1 (Lexical):\")\n",
    "#     retrieval_counts1 = results_df['lexical_retrieval'].value_counts()\n",
    "#     for method, count in retrieval_counts1.items():\n",
    "#         pct = count / len(results_df) * 100\n",
    "#         print(f\"  {method}: {count} ({pct:.1f}%)\")\n",
    "    \n",
    "#     print(\"\\nMethod 2 (Semantic):\")\n",
    "#     retrieval_counts2 = results_df['semantic_retrieval'].value_counts()\n",
    "#     for method, count in retrieval_counts2.items():\n",
    "#         pct = count / len(results_df) * 100\n",
    "#         print(f\"  {method}: {count} ({pct:.1f}%)\")\n",
    "    \n",
    "#     # Save results\n",
    "#     output_path = csv_path.replace('.csv', '_comparison_lexical_vs_semantic.csv')\n",
    "#     results_df.to_csv(output_path, index=False, encoding='utf-8-sig')\n",
    "#     print(f\"\\n‚úÖ Results saved to: {output_path}\")\n",
    "    \n",
    "#     # Error analysis - Cases where methods differ\n",
    "#     print(\"\\n\" + \"=\" * 80)\n",
    "#     print(\"CASES WHERE METHODS DIFFER\")\n",
    "#     print(\"=\" * 80)\n",
    "    \n",
    "#     diff_results = results_df[results_df['lexical_pred'] != results_df['semantic_pred']]\n",
    "#     print(f\"\\nFound {len(diff_results)} cases where predictions differ\")\n",
    "    \n",
    "#     # Cases where semantic is correct but lexical is wrong\n",
    "#     semantic_wins = results_df[\n",
    "#         (~results_df['lexical_correct']) & results_df['semantic_correct']\n",
    "#     ]\n",
    "#     print(f\"\\nüéØ Semantic correct, Lexical wrong: {len(semantic_wins)} cases\")\n",
    "    \n",
    "#     for idx, row in semantic_wins.head(5).iterrows():\n",
    "#         print(f\"\\n  [{idx+1}] Query: '{row['query']}'\")\n",
    "#         print(f\"      Expected: '{row['expected']}'\")\n",
    "#         print(f\"      Lexical:  '{row['lexical_pred']}' [{row['lexical_score']:.3f}] ‚ùå\")\n",
    "#         print(f\"      Semantic: '{row['semantic_pred']}' [{row['semantic_score']:.3f}] ‚úÖ\")\n",
    "    \n",
    "#     # Cases where lexical is correct but semantic is wrong\n",
    "#     lexical_wins = results_df[\n",
    "#         results_df['lexical_correct'] & (~results_df['semantic_correct'])\n",
    "#     ]\n",
    "#     print(f\"\\nüéØ Lexical correct, Semantic wrong: {len(lexical_wins)} cases\")\n",
    "    \n",
    "#     for idx, row in lexical_wins.head(5).iterrows():\n",
    "#         print(f\"\\n  [{idx+1}] Query: '{row['query']}'\")\n",
    "#         print(f\"      Expected: '{row['expected']}'\")\n",
    "#         print(f\"      Lexical:  '{row['lexical_pred']}' [{row['lexical_score']:.3f}] ‚úÖ\")\n",
    "#         print(f\"      Semantic: '{row['semantic_pred']}' [{row['semantic_score']:.3f}] ‚ùå\")\n",
    "    \n",
    "#     # Agreement analysis\n",
    "#     print(\"\\n\" + \"=\" * 80)\n",
    "#     print(\"AGREEMENT ANALYSIS\")\n",
    "#     print(\"=\" * 80)\n",
    "    \n",
    "#     both_correct = results_df[\n",
    "#         results_df['lexical_correct'] & results_df['semantic_correct']\n",
    "#     ]\n",
    "#     print(f\"\\nBoth methods correct: {len(both_correct)}/{len(results_df)} ({len(both_correct)/len(results_df)*100:.1f}%)\")\n",
    "    \n",
    "#     both_wrong_same = results_df[\n",
    "#         (~results_df['lexical_correct']) & \n",
    "#         (~results_df['semantic_correct']) &\n",
    "#         (results_df['lexical_pred'] == results_df['semantic_pred'])\n",
    "#     ]\n",
    "#     print(f\"Both wrong (same answer): {len(both_wrong_same)}/{len(results_df)} ({len(both_wrong_same)/len(results_df)*100:.1f}%)\")\n",
    "    \n",
    "#     both_wrong_diff = results_df[\n",
    "#         (~results_df['lexical_correct']) & \n",
    "#         (~results_df['semantic_correct']) &\n",
    "#         (results_df['lexical_pred'] != results_df['semantic_pred'])\n",
    "#     ]\n",
    "#     print(f\"Both wrong (different answer): {len(both_wrong_diff)}/{len(results_df)} ({len(both_wrong_diff)/len(results_df)*100:.1f}%)\")\n",
    "    \n",
    "#     return results_df\n",
    "\n",
    "\n",
    "# # ==== MAIN ====\n",
    "# if __name__ == \"__main__\":\n",
    "#     csv_path = \"/data/small-language-models/duc/Searching_cate/data/cate_long.csv\"\n",
    "    \n",
    "#     results = compare_all_methods(\n",
    "#         csv_path=csv_path,\n",
    "#         device=\"cuda\",  # Ho·∫∑c \"cuda\"\n",
    "#         verbose=False   # Set True ƒë·ªÉ xem chi ti·∫øt t·ª´ng test case\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d2c1b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "from typing import List, Tuple, Optional, Dict, Set\n",
    "import re\n",
    "from dataclasses import dataclass\n",
    "from enum import Enum\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoTokenizer, AutoModel, AutoModelForCausalLM\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "class CategoryLevel(Enum):\n",
    "    \"\"\"Ph√¢n lo·∫°i m·ª©c ƒë·ªô ∆∞u ti√™n c·ªßa category\"\"\"\n",
    "    PRODUCT_TYPE = 1      # Lo·∫°i s·∫£n ph·∫©m c·ª• th·ªÉ (quan tr·ªçng nh·∫•t)\n",
    "    ATTRIBUTES = 2        # ƒê·∫∑c t√≠nh s·∫£n ph·∫©m\n",
    "    BRAND = 3            # Th∆∞∆°ng hi·ªáu\n",
    "    GENERAL = 4          # Danh m·ª•c chung\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Product:\n",
    "    id: str\n",
    "    name: str\n",
    "    category_path: List[str]\n",
    "    level: CategoryLevel = CategoryLevel.GENERAL\n",
    "    priority_score: float = 0.0\n",
    "\n",
    "\n",
    "class PrefixTreeNode:\n",
    "    def __init__(self, token: str):\n",
    "        self.token = token\n",
    "        self.children = {}\n",
    "        self.products = []\n",
    "    \n",
    "    def add_child(self, token: str):\n",
    "        if token not in self.children:\n",
    "            self.children[token] = PrefixTreeNode(token)\n",
    "        return self.children[token]\n",
    "\n",
    "\n",
    "class SemanticEncoder:\n",
    "    \"\"\"Encoder ƒë·ªÉ t·∫°o embeddings cho semantic search\"\"\"\n",
    "    def __init__(self, model_name: str = \"Qwen/Qwen3-Embedding-0.6B\", device: str = None):\n",
    "        print(\"Loading Semantic Encoder...\")\n",
    "        if device is None:\n",
    "            device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        \n",
    "        self.device = device\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = AutoModel.from_pretrained(model_name).to(device).eval()\n",
    "        print(f\"‚úÖ Semantic Encoder loaded on {device}\")\n",
    "    \n",
    "    def encode(self, texts: List[str]) -> np.ndarray:\n",
    "        \"\"\"Encode texts th√†nh embeddings\"\"\"\n",
    "        with torch.no_grad():\n",
    "            inputs = self.tokenizer(texts, padding=True, truncation=True, \n",
    "                                  return_tensors=\"pt\", max_length=512)\n",
    "            inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
    "            \n",
    "            outputs = self.model(**inputs)\n",
    "            embeddings = outputs.last_hidden_state.mean(dim=1)\n",
    "            embeddings = F.normalize(embeddings, p=2, dim=1)\n",
    "            \n",
    "        return embeddings.cpu().numpy()\n",
    "\n",
    "\n",
    "class Qwen3CrossEncoder:\n",
    "    def __init__(self, model_name: str = \"Qwen/Qwen3-Reranker-0.6B\", device: str = None):\n",
    "        print(\"Loading Qwen3-Reranker...\")\n",
    "        if device is None:\n",
    "            device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        \n",
    "        self.device = device\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name, padding_side='left')\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(model_name).to(device).eval()\n",
    "        \n",
    "        self.token_false_id = self.tokenizer.convert_tokens_to_ids(\"no\")\n",
    "        self.token_true_id = self.tokenizer.convert_tokens_to_ids(\"yes\")\n",
    "        self.max_length = 8192\n",
    "        \n",
    "        self.prefix = \"<|im_start|>system\\nJudge whether the Document meets the requirements based on the Query and the Instruct provided. Note that the answer can only be \\\"yes\\\" or \\\"no\\\".<|im_end|>\\n<|im_start|>user\\n\"\n",
    "        self.suffix = \"<|im_end|>\\n<|im_start|>assistant\\n<think>\\n\\n</think>\\n\\n\"\n",
    "        self.prefix_tokens = self.tokenizer.encode(self.prefix, add_special_tokens=False)\n",
    "        self.suffix_tokens = self.tokenizer.encode(self.suffix, add_special_tokens=False)\n",
    "        \n",
    "        self.task_instruction = \"Given a product search query, judge whether the product matches the user's search intent\"\n",
    "        print(f\"‚úÖ Cross-encoder loaded on {device}\")\n",
    "    \n",
    "    def format_instruction(self, query: str, doc: str) -> str:\n",
    "        return \"<Instruct>: {instruction}\\n<Query>: {query}\\n<Document>: {doc}\".format(\n",
    "            instruction=self.task_instruction, query=query, doc=doc\n",
    "        )\n",
    "    \n",
    "    def process_inputs(self, pairs: List[str]):\n",
    "        inputs = self.tokenizer(\n",
    "            pairs, padding=False, truncation='longest_first',\n",
    "            return_attention_mask=False, \n",
    "            max_length=self.max_length - len(self.prefix_tokens) - len(self.suffix_tokens)\n",
    "        )\n",
    "        \n",
    "        for i, ele in enumerate(inputs['input_ids']):\n",
    "            inputs['input_ids'][i] = self.prefix_tokens + ele + self.suffix_tokens\n",
    "        \n",
    "        inputs = self.tokenizer.pad(inputs, padding=True, return_tensors=\"pt\", max_length=self.max_length)\n",
    "        \n",
    "        for key in inputs:\n",
    "            inputs[key] = inputs[key].to(self.device)\n",
    "        \n",
    "        return inputs\n",
    "    \n",
    "    def compute_logits(self, inputs):\n",
    "        with torch.no_grad():\n",
    "            batch_scores = self.model(**inputs).logits[:, -1, :]\n",
    "            true_vector = batch_scores[:, self.token_true_id]\n",
    "            false_vector = batch_scores[:, self.token_false_id]\n",
    "            batch_scores = torch.stack([false_vector, true_vector], dim=1)\n",
    "            batch_scores = torch.nn.functional.log_softmax(batch_scores, dim=1)\n",
    "            scores = batch_scores[:, 1].exp().tolist()\n",
    "        return scores\n",
    "    \n",
    "    def predict(self, pairs: List[List[str]]) -> List[float]:\n",
    "        formatted_pairs = [self.format_instruction(query, doc) for query, doc in pairs]\n",
    "        inputs = self.process_inputs(formatted_pairs)\n",
    "        scores = self.compute_logits(inputs)\n",
    "        return scores\n",
    "\n",
    "\n",
    "def parse_categories(category_str: str) -> List[str]:\n",
    "    \"\"\"Parse danh m·ª•c t·ª´ string\"\"\"\n",
    "    category_str = category_str.strip()\n",
    "    if category_str.startswith('[') and category_str.endswith(']'):\n",
    "        category_str = category_str[1:-1]\n",
    "    categories = [cat.strip() for cat in category_str.split(',')]\n",
    "    return categories\n",
    "\n",
    "\n",
    "def normalize_text(text: str) -> List[str]:\n",
    "    \"\"\"Chu·∫©n h√≥a v√† t√°ch t·ª´\"\"\"\n",
    "    text = text.lower()\n",
    "    tokens = re.findall(r'\\w+', text)\n",
    "    return tokens\n",
    "\n",
    "\n",
    "def classify_category_level(category: str) -> CategoryLevel:\n",
    "    \"\"\"\n",
    "    Ph√¢n lo·∫°i category theo m·ª©c ƒë·ªô ∆∞u ti√™n\n",
    "    \"\"\"\n",
    "    category_lower = category.lower()\n",
    "    \n",
    "    # PRODUCT_TYPE: Lo·∫°i s·∫£n ph·∫©m c·ª• th·ªÉ (cao nh·∫•t)\n",
    "    product_type_keywords = [\n",
    "        's·ªØa r·ª≠a m·∫∑t', 'gel r·ª≠a m·∫∑t', 't·∫°o b·ªçt', 'foaming', \n",
    "        'd·∫°ng kem', 'd·∫°ng gel', 'cleanser', 'toner', 'serum',\n",
    "        'kem d∆∞·ª°ng', 'kem ch·ªëng n·∫Øng', 'm·∫∑t n·∫°', 't·∫©y trang',\n",
    "        'n∆∞·ªõc hoa h·ªìng', 't·∫©y t·∫ø b√†o ch·∫øt', 'x·ªãt kho√°ng',\n",
    "        's·ªØa t·∫Øm', 'd·∫ßu g·ªôi', 'd·∫ßu x·∫£', 'd∆∞·ª°ng th·ªÉ',\n",
    "        'son m√¥i', 'mascara', 'kem n·ªÅn', 'ph·∫•n ph·ªß'\n",
    "    ]\n",
    "    \n",
    "    for keyword in product_type_keywords:\n",
    "        if keyword in category_lower:\n",
    "            return CategoryLevel.PRODUCT_TYPE\n",
    "    \n",
    "    # ATTRIBUTES: ƒê·∫∑c t√≠nh s·∫£n ph·∫©m\n",
    "    attribute_keywords = [\n",
    "        'cho da d·∫ßu', 'cho da kh√¥', 'cho da nh·∫°y c·∫£m', 'cho da m·ª•n',\n",
    "        'ki·ªÅm d·∫ßu', 'c·∫•p ·∫©m', 'l√†m s·∫°ch s√¢u', 'd∆∞·ª°ng tr·∫Øng',\n",
    "        'ch·ªëng l√£o h√≥a', 'gi·∫£m m·ª•n', 'se kh√≠t l·ªó ch√¢n l√¥ng',\n",
    "        'kh√¥ng ch·ª©a c·ªìn', 'kh√¥ng ch·ª©a paraben', 'organic',\n",
    "        'thi√™n nhi√™n', 'ch·ª©a bha', 'salicylic acid'\n",
    "    ]\n",
    "    \n",
    "    for keyword in attribute_keywords:\n",
    "        if keyword in category_lower:\n",
    "            return CategoryLevel.ATTRIBUTES\n",
    "    \n",
    "    # BRAND: Th∆∞∆°ng hi·ªáu\n",
    "    brand_keywords = [\n",
    "        'la roche-posay', 'laroche', 'effaclar', 'vichy', \n",
    "        'eucerin', 'cetaphil', 'simple', 'loreal', 'garnier',\n",
    "        'innisfree', 'some by mi', 'cocoon'\n",
    "    ]\n",
    "    \n",
    "    for keyword in brand_keywords:\n",
    "        if keyword in category_lower:\n",
    "            return CategoryLevel.BRAND\n",
    "    \n",
    "    # GENERAL: M·∫∑c ƒë·ªãnh\n",
    "    return CategoryLevel.GENERAL\n",
    "\n",
    "\n",
    "def calculate_priority_score(product: Product, query_tokens: List[str]) -> float:\n",
    "    \"\"\"\n",
    "    T√≠nh ƒëi·ªÉm ∆∞u ti√™n d·ª±a tr√™n:\n",
    "    1. Category level (quan tr·ªçng nh·∫•t)\n",
    "    2. Token matching v·ªõi query\n",
    "    \"\"\"\n",
    "    # Base score theo level\n",
    "    level_scores = {\n",
    "        CategoryLevel.PRODUCT_TYPE: 100.0,\n",
    "        CategoryLevel.ATTRIBUTES: 50.0,\n",
    "        CategoryLevel.BRAND: 30.0,\n",
    "        CategoryLevel.GENERAL: 10.0\n",
    "    }\n",
    "    \n",
    "    base_score = level_scores[product.level]\n",
    "    \n",
    "    # Bonus t·ª´ token matching\n",
    "    category_tokens = normalize_text(product.name)\n",
    "    matches = sum(1 for qt in query_tokens if qt in category_tokens)\n",
    "    match_bonus = matches * 5\n",
    "    \n",
    "    return base_score + match_bonus\n",
    "\n",
    "\n",
    "def create_prioritized_products(categories: List[str], idx: int) -> List[Product]:\n",
    "    \"\"\"T·∫°o products v·ªõi ph√¢n lo·∫°i level v√† priority score\"\"\"\n",
    "    products = []\n",
    "    \n",
    "    for i, cat in enumerate(categories):\n",
    "        level = classify_category_level(cat)\n",
    "        product = Product(\n",
    "            id=f\"{idx}_{i}\",\n",
    "            name=cat,\n",
    "            category_path=[cat],\n",
    "            level=level,\n",
    "            priority_score=0.0  # S·∫Ω ƒë∆∞·ª£c t√≠nh sau\n",
    "        )\n",
    "        products.append(product)\n",
    "    \n",
    "    return products\n",
    "\n",
    "\n",
    "def build_prefix_tree(products: List[Product]) -> PrefixTreeNode:\n",
    "    \"\"\"X√¢y d·ª±ng prefix tree\"\"\"\n",
    "    root = PrefixTreeNode(\"ROOT\")\n",
    "    \n",
    "    for product in products:\n",
    "        current = root\n",
    "        full_path = \" \".join(product.category_path)\n",
    "        tokens = normalize_text(full_path)\n",
    "        \n",
    "        # Lo·∫°i b·ªè duplicate li√™n ti·∫øp\n",
    "        unique_tokens = []\n",
    "        for token in tokens:\n",
    "            if not unique_tokens or token != unique_tokens[-1]:\n",
    "                unique_tokens.append(token)\n",
    "        \n",
    "        for token in unique_tokens:\n",
    "            current = current.add_child(token)\n",
    "        \n",
    "        current.products.append(product)\n",
    "    \n",
    "    return root\n",
    "\n",
    "\n",
    "def find_matching_node(tree: PrefixTreeNode, query: str) -> Optional[PrefixTreeNode]:\n",
    "    \"\"\"T√¨m node s√¢u nh·∫•t match v·ªõi query\"\"\"\n",
    "    query_tokens = normalize_text(query)\n",
    "    \n",
    "    current = tree\n",
    "    deepest_with_products = None\n",
    "    \n",
    "    for token in query_tokens:\n",
    "        if token in current.children:\n",
    "            current = current.children[token]\n",
    "            if current.products:\n",
    "                deepest_with_products = current\n",
    "        else:\n",
    "            break\n",
    "    \n",
    "    return deepest_with_products\n",
    "\n",
    "\n",
    "def filter_by_priority(products: List[Product], query_tokens: List[str]) -> List[Product]:\n",
    "    \"\"\"\n",
    "    L·ªçc candidates theo priority:\n",
    "    1. ∆Øu ti√™n PRODUCT_TYPE\n",
    "    2. N·∫øu kh√¥ng c√≥, l·∫•y level cao nh·∫•t c√≥ s·∫µn\n",
    "    3. Trong c√πng level, sort theo token matching\n",
    "    \"\"\"\n",
    "    if not products:\n",
    "        return []\n",
    "    \n",
    "    # T√≠nh priority score cho t·∫•t c·∫£\n",
    "    for p in products:\n",
    "        p.priority_score = calculate_priority_score(p, query_tokens)\n",
    "    \n",
    "    # Nh√≥m theo level\n",
    "    by_level: Dict[CategoryLevel, List[Product]] = {}\n",
    "    for p in products:\n",
    "        if p.level not in by_level:\n",
    "            by_level[p.level] = []\n",
    "        by_level[p.level].append(p)\n",
    "    \n",
    "    # ∆Øu ti√™n PRODUCT_TYPE\n",
    "    if CategoryLevel.PRODUCT_TYPE in by_level:\n",
    "        candidates = by_level[CategoryLevel.PRODUCT_TYPE]\n",
    "    # N·∫øu kh√¥ng c√≥ PRODUCT_TYPE, l·∫•y level cao nh·∫•t\n",
    "    else:\n",
    "        best_level = min(by_level.keys(), key=lambda x: x.value)\n",
    "        candidates = by_level[best_level]\n",
    "    \n",
    "    # Sort theo priority score\n",
    "    candidates.sort(key=lambda x: x.priority_score, reverse=True)\n",
    "    \n",
    "    return candidates\n",
    "\n",
    "\n",
    "def lexical_search(query: str, products: List[Product], top_n: int = 50) -> List[Product]:\n",
    "    \"\"\"Lexical search v·ªõi priority filtering\"\"\"\n",
    "    query_tokens = normalize_text(query)\n",
    "    \n",
    "    scores = []\n",
    "    for product in products:\n",
    "        all_text = product.name + \" \" + \" \".join(product.category_path)\n",
    "        product_tokens = normalize_text(all_text)\n",
    "        \n",
    "        matches = sum(1 for qt in query_tokens if qt in product_tokens)\n",
    "        score = matches / len(query_tokens) if query_tokens else 0\n",
    "        \n",
    "        if score > 0:\n",
    "            scores.append((product, score))\n",
    "    \n",
    "    # Sort theo score\n",
    "    scores.sort(key=lambda x: x[1], reverse=True)\n",
    "    candidates = [p for p, s in scores[:top_n]]\n",
    "    \n",
    "    # Apply priority filtering\n",
    "    return filter_by_priority(candidates, query_tokens)[:top_n]\n",
    "\n",
    "\n",
    "def semantic_search(query: str, products: List[Product], \n",
    "                    semantic_encoder: SemanticEncoder, top_n: int = 50) -> List[Product]:\n",
    "    \"\"\"Semantic search v·ªõi priority filtering\"\"\"\n",
    "    query_tokens = normalize_text(query)\n",
    "    query_emb = semantic_encoder.encode([query])[0]\n",
    "    \n",
    "    texts = [p.name for p in products]\n",
    "    product_embs = semantic_encoder.encode(texts)\n",
    "    \n",
    "    similarities = [float(np.dot(query_emb, emb)) for emb in product_embs]\n",
    "    \n",
    "    scored = list(zip(products, similarities))\n",
    "    scored.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    candidates = [p for p, s in scored[:top_n]]\n",
    "    \n",
    "    # Apply priority filtering\n",
    "    return filter_by_priority(candidates, query_tokens)[:top_n]\n",
    "\n",
    "\n",
    "def method_prefix_lexical_cross_v2(query: str, categories: List[str],\n",
    "                                    cross_encoder: Qwen3CrossEncoder,\n",
    "                                    idx: int) -> Tuple[str, float, str, str]:\n",
    "    \"\"\"\n",
    "    Ph∆∞∆°ng ph√°p c·∫£i ti·∫øn v·ªõi Priority System:\n",
    "    Prefix Tree ‚Üí Priority Filtering ‚Üí Lexical Search ‚Üí Cross-encoder\n",
    "    \"\"\"\n",
    "    query_tokens = normalize_text(query)\n",
    "    \n",
    "    # T·∫°o products v·ªõi level classification\n",
    "    products = create_prioritized_products(categories, idx)\n",
    "    \n",
    "    # Build prefix tree\n",
    "    tree = build_prefix_tree(products)\n",
    "    \n",
    "    # B∆∞·ªõc 1: Prefix matching\n",
    "    matched_node = find_matching_node(tree, query)\n",
    "    \n",
    "    if matched_node and matched_node.products:\n",
    "        # B∆∞·ªõc 2: Priority filtering\n",
    "        candidates = filter_by_priority(matched_node.products, query_tokens)\n",
    "        retrieval_method = \"prefix+priority\"\n",
    "        level_info = candidates[0].level.name if candidates else \"NONE\"\n",
    "    else:\n",
    "        # B∆∞·ªõc 3: Lexical search (fallback) v·ªõi priority\n",
    "        candidates = lexical_search(query, products, top_n=50)\n",
    "        retrieval_method = \"lexical+priority\"\n",
    "        level_info = candidates[0].level.name if candidates else \"NONE\"\n",
    "    \n",
    "    if not candidates:\n",
    "        return categories[0], 0.0, \"no_candidates\", \"NONE\"\n",
    "    \n",
    "    # B∆∞·ªõc 4: Cross-encoder\n",
    "    pairs = [[query, p.name] for p in candidates]\n",
    "    scores = cross_encoder.predict(pairs)\n",
    "    \n",
    "    best_idx = np.argmax(scores)\n",
    "    return candidates[best_idx].name, scores[best_idx], retrieval_method, level_info\n",
    "\n",
    "\n",
    "def method_prefix_semantic_cross_v2(query: str, categories: List[str],\n",
    "                                     semantic_encoder: SemanticEncoder,\n",
    "                                     cross_encoder: Qwen3CrossEncoder,\n",
    "                                     idx: int) -> Tuple[str, float, str, str]:\n",
    "    \"\"\"\n",
    "    Ph∆∞∆°ng ph√°p c·∫£i ti·∫øn v·ªõi Priority System:\n",
    "    Prefix Tree ‚Üí Priority Filtering ‚Üí Semantic Search ‚Üí Cross-encoder\n",
    "    \"\"\"\n",
    "    query_tokens = normalize_text(query)\n",
    "    \n",
    "    # T·∫°o products v·ªõi level classification\n",
    "    products = create_prioritized_products(categories, idx)\n",
    "    \n",
    "    # Build prefix tree\n",
    "    tree = build_prefix_tree(products)\n",
    "    \n",
    "    # B∆∞·ªõc 1: Prefix matching\n",
    "    matched_node = find_matching_node(tree, query)\n",
    "    \n",
    "    if matched_node and matched_node.products:\n",
    "        # B∆∞·ªõc 2: Priority filtering\n",
    "        candidates = filter_by_priority(matched_node.products, query_tokens)\n",
    "        retrieval_method = \"prefix+priority\"\n",
    "        level_info = candidates[0].level.name if candidates else \"NONE\"\n",
    "    else:\n",
    "        # B∆∞·ªõc 3: Semantic search (fallback) v·ªõi priority\n",
    "        candidates = semantic_search(query, products, semantic_encoder, top_n=50)\n",
    "        retrieval_method = \"semantic+priority\"\n",
    "        level_info = candidates[0].level.name if candidates else \"NONE\"\n",
    "    \n",
    "    if not candidates:\n",
    "        return categories[0], 0.0, \"no_candidates\", \"NONE\"\n",
    "    \n",
    "    # B∆∞·ªõc 4: Cross-encoder\n",
    "    pairs = [[query, p.name] for p in candidates]\n",
    "    scores = cross_encoder.predict(pairs)\n",
    "    \n",
    "    best_idx = np.argmax(scores)\n",
    "    return candidates[best_idx].name, scores[best_idx], retrieval_method, level_info\n",
    "\n",
    "\n",
    "def compare_all_methods_v2(csv_path: str, device: str = \"cpu\", verbose: bool = False):\n",
    "    \"\"\"\n",
    "    So s√°nh ph∆∞∆°ng ph√°p c≈© vs m·ªõi (v·ªõi Priority System):\n",
    "    1. Old: Prefix ‚Üí Lexical ‚Üí Cross\n",
    "    2. New: Prefix ‚Üí Priority ‚Üí Lexical ‚Üí Cross\n",
    "    3. New: Prefix ‚Üí Priority ‚Üí Semantic ‚Üí Cross\n",
    "    \"\"\"\n",
    "    print(\"=\" * 80)\n",
    "    print(\"COMPARING: Old vs New (WITH PRIORITY SYSTEM)\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Load data\n",
    "    df = pd.read_csv(csv_path)\n",
    "    print(f\"\\n‚úÖ Loaded {len(df)} test cases\")\n",
    "    \n",
    "    # Initialize models\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"INITIALIZING MODELS\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    semantic_encoder = SemanticEncoder(device=device)\n",
    "    cross_encoder = Qwen3CrossEncoder(device=device)\n",
    "    \n",
    "    # Results storage\n",
    "    results = []\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"RUNNING TESTS\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Progress bar\n",
    "    for idx, row in tqdm(df.iterrows(), total=len(df), desc=\"Processing\"):\n",
    "        query = row['t√™n h√†ng h√≥a c·∫ßn map']\n",
    "        expected = row['k·∫øt qu·∫£ mong mu·ªën']\n",
    "        categories_str = row['danh m·ª•c']\n",
    "        categories = parse_categories(categories_str)\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"\\n--- Test {idx + 1}/{len(df)} ---\")\n",
    "            print(f\"Query: {query}\")\n",
    "            print(f\"Expected: {expected}\")\n",
    "        \n",
    "        # NEW METHOD 1: Priority + Lexical\n",
    "        pred1, score1, retrieval1, level1 = method_prefix_lexical_cross_v2(\n",
    "            query, categories, cross_encoder, idx\n",
    "        )\n",
    "        correct1 = (pred1.strip().lower() == expected.strip().lower())\n",
    "        \n",
    "        # NEW METHOD 2: Priority + Semantic\n",
    "        pred2, score2, retrieval2, level2 = method_prefix_semantic_cross_v2(\n",
    "            query, categories, semantic_encoder, cross_encoder, idx\n",
    "        )\n",
    "        correct2 = (pred2.strip().lower() == expected.strip().lower())\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"  Priority+Lexical:  {pred1} [{score1:.3f}] via {retrieval1} [{level1}] {'‚úÖ' if correct1 else '‚ùå'}\")\n",
    "            print(f\"  Priority+Semantic: {pred2} [{score2:.3f}] via {retrieval2} [{level2}] {'‚úÖ' if correct2 else '‚ùå'}\")\n",
    "        \n",
    "        results.append({\n",
    "            'query': query,\n",
    "            'categories': str(categories),\n",
    "            'expected': expected,\n",
    "            \n",
    "            # Priority + Lexical\n",
    "            'priority_lexical_pred': pred1,\n",
    "            'priority_lexical_score': score1,\n",
    "            'priority_lexical_retrieval': retrieval1,\n",
    "            'priority_lexical_level': level1,\n",
    "            'priority_lexical_correct': correct1,\n",
    "            \n",
    "            # Priority + Semantic\n",
    "            'priority_semantic_pred': pred2,\n",
    "            'priority_semantic_score': score2,\n",
    "            'priority_semantic_retrieval': retrieval2,\n",
    "            'priority_semantic_level': level2,\n",
    "            'priority_semantic_correct': correct2,\n",
    "        })\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    results_df = pd.DataFrame(results)\n",
    "    \n",
    "    # Calculate accuracies\n",
    "    acc1 = (results_df['priority_lexical_correct'].sum() / len(results_df)) * 100\n",
    "    acc2 = (results_df['priority_semantic_correct'].sum() / len(results_df)) * 100\n",
    "    \n",
    "    # Print summary\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"FINAL RESULTS (WITH PRIORITY SYSTEM)\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    print(f\"\\n{'Method':<60} {'Accuracy':<12} {'Correct/Total'}\")\n",
    "    print(\"-\" * 80)\n",
    "    print(f\"{'1. Prefix ‚Üí Priority ‚Üí Lexical ‚Üí Cross':<60} {acc1:>6.2f}%     {results_df['priority_lexical_correct'].sum():>3}/{len(results_df)}\")\n",
    "    print(f\"{'2. Prefix ‚Üí Priority ‚Üí Semantic ‚Üí Cross':<60} {acc2:>6.2f}%     {results_df['priority_semantic_correct'].sum():>3}/{len(results_df)}\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    improvement = acc2 - acc1\n",
    "    print(f\"\\nüìä Semantic over Lexical (with Priority): {improvement:>+6.2f}%\")\n",
    "    \n",
    "    # Level distribution\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"CATEGORY LEVEL DISTRIBUTION\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    print(\"\\nMethod 1 (Priority + Lexical):\")\n",
    "    level_counts1 = results_df['priority_lexical_level'].value_counts()\n",
    "    for level, count in level_counts1.items():\n",
    "        pct = count / len(results_df) * 100\n",
    "        correct_at_level = results_df[\n",
    "            (results_df['priority_lexical_level'] == level) & \n",
    "            results_df['priority_lexical_correct']\n",
    "        ].shape[0]\n",
    "        acc_at_level = correct_at_level / count * 100 if count > 0 else 0\n",
    "        print(f\"  {level}: {count} ({pct:.1f}%) - Acc: {acc_at_level:.1f}%\")\n",
    "    \n",
    "    print(\"\\nMethod 2 (Priority + Semantic):\")\n",
    "    level_counts2 = results_df['priority_semantic_level'].value_counts()\n",
    "    for level, count in level_counts2.items():\n",
    "        pct = count / len(results_df) * 100\n",
    "        correct_at_level = results_df[\n",
    "            (results_df['priority_semantic_level'] == level) & \n",
    "            results_df['priority_semantic_correct']\n",
    "        ].shape[0]\n",
    "        acc_at_level = correct_at_level / count * 100 if count > 0 else 0\n",
    "        print(f\"  {level}: {count} ({pct:.1f}%) - Acc: {acc_at_level:.1f}%\")\n",
    "    \n",
    "    # Save results\n",
    "    output_path = csv_path.replace('.csv', '_priority_system_comparison.csv')\n",
    "    results_df.to_csv(output_path, index=False, encoding='utf-8-sig')\n",
    "    print(f\"\\n‚úÖ Results saved to: {output_path}\")\n",
    "    \n",
    "    # Error analysis\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"KEY IMPROVEMENTS FROM PRIORITY SYSTEM\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Cases improved by priority system\n",
    "    improved_cases = results_df[\n",
    "        results_df['priority_lexical_correct'] | results_df['priority_semantic_correct']\n",
    "    ]\n",
    "    \n",
    "    product_type_wins = improved_cases[\n",
    "        (improved_cases['priority_lexical_level'] == 'PRODUCT_TYPE') |\n",
    "        (improved_cases['priority_semantic_level'] == 'PRODUCT_TYPE')\n",
    "    ]\n",
    "    \n",
    "    print(f\"\\n‚úÖ Cases where PRODUCT_TYPE was correctly prioritized: {len(product_type_wins)}\")\n",
    "    \n",
    "    for idx, row in product_type_wins.head(3).iterrows():\n",
    "        print(f\"\\n  [{idx+1}] Query: '{row['query']}'\")\n",
    "        print(f\"      Expected: '{row['expected']}'\")\n",
    "        print(f\"      Priority+Lexical:  '{row['priority_lexical_pred']}' [{row['priority_lexical_level']}] {'‚úÖ' if row['priority_lexical_correct'] else '‚ùå'}\")\n",
    "        print(f\"      Priority+Semantic: '{row['priority_semantic_pred']}' [{row['priority_semantic_level']}] {'‚úÖ' if row['priority_semantic_correct'] else '‚ùå'}\")\n",
    "    \n",
    "    return results_df\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Test v·ªõi file CSV\n",
    "    csv_path = \"test_cases.csv\"  # Thay b·∫±ng path th·ª±c t·∫ø\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    \n",
    "    results = compare_all_methods_v2(\n",
    "        csv_path=csv_path,\n",
    "        device=device,\n",
    "        verbose=True  # Set False ƒë·ªÉ ch·∫°y nhanh h∆°n\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "duc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
